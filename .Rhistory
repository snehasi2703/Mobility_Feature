MaxDiam_km_max_7day = rollapplyr(MaxDiam_km,width=7,max,partial=T,na.rm=T),
MaxHomeDist_km_max_7day = rollapplyr(MaxHomeDist_km,width=7,max,partial=T,na.rm=T),
AvgFlightLen_km_max_7day = rollapplyr(AvgFlightLen_km,width=7,max,partial=T,na.rm=T),
StdFlightLen_km_max_7day = rollapplyr(StdFlightLen_km,width=7,max,partial=T,na.rm=T),
AvgFlightDur_min_max_7day = rollapplyr(AvgFlightDur_min,width=7,max,partial=T,na.rm=T),
StdFlightDur_min_max_7day = rollapplyr(StdFlightDur_min,width=7,max,partial=T,na.rm=T),
Hometime_hrs_max_7day = rollapplyr(Hometime_hrs,width=7,max,partial=T,na.rm=T),
SigLocsVisited_max_7day = rollapplyr(SigLocsVisited,width=7,max,partial=T,na.rm=T),
SigLocEntropy_max_7day = rollapplyr(SigLocEntropy,width=7,max,partial=T,na.rm=T),
ProbPause_max_7day = rollapplyr(ProbPause,width=7,max,partial=T,na.rm=T),
CircdnRtn_max_7day = rollapplyr(CircdnRtn,width=7,max,partial=T,na.rm=T),
WkEndDayRtn_max_7day = rollapplyr(WkEndDayRtn,width=7,max,partial=T,na.rm=T),
RoG_km_max_7day = rollapplyr(RoG_km,width=7,max,partial=T,na.rm=T)
)
metric_summary_temp3 <- metric_summary_temp2 %>%
group_by(person_id) %>%
mutate(DistTravelled_km_min_7day = rollapplyr(DistTravelled_km,width=7,min,partial=T,na.rm=T),
MaxDiam_km_min_7day = rollapplyr(MaxDiam_km,width=7,min,partial=T,na.rm=T),
MaxHomeDist_km_min_7day = rollapplyr(MaxHomeDist_km,width=7,min,partial=T,na.rm=T),
AvgFlightLen_km_min_7day = rollapplyr(AvgFlightLen_km,width=7,min,partial=T,na.rm=T),
StdFlightLen_km_min_7day = rollapplyr(StdFlightLen_km,width=7,min,partial=T,na.rm=T),
AvgFlightDur_min_min_7day = rollapplyr(AvgFlightDur_min,width=7,min,partial=T,na.rm=T),
StdFlightDur_min_min_7day = rollapplyr(StdFlightDur_min,width=7,min,partial=T,na.rm=T),
Hometime_hrs_min_7day = rollapplyr(Hometime_hrs,width=7,min,partial=T,na.rm=T),
SigLocsVisited_min_7day = rollapplyr(SigLocsVisited,width=7,min,partial=T,na.rm=T),
SigLocEntropy_min_7day = rollapplyr(SigLocEntropy,width=7,min,partial=T,na.rm=T),
ProbPause_min_7day = rollapplyr(ProbPause,width=7,min,partial=T,na.rm=T),
CircdnRtn_min_7day = rollapplyr(CircdnRtn,width=7,min,partial=T,na.rm=T),
WkEndDayRtn_min_7day = rollapplyr(WkEndDayRtn,width=7,min,partial=T,na.rm=T),
RoG_km_min_7day = rollapplyr(RoG_km,width=7,min,partial=T,na.rm=T))
metric_summary_weekly_lags <- metric_summary_temp3 %>%
group_by(person_id) %>%
mutate(DistTravelled_km_med_7day = rollapplyr(DistTravelled_km,width=7,median,partial=T,na.rm=T),
MaxDiam_km_med_7day = rollapplyr(MaxDiam_km,width=7,median,partial=T,na.rm=T),
MaxHomeDist_km_med_7day = rollapplyr(MaxHomeDist_km,width=7,median,partial=T,na.rm=T),
AvgFlightLen_km_med_7day = rollapplyr(AvgFlightLen_km,width=7,median,partial=T,na.rm=T),
StdFlightLen_km_med_7day = rollapplyr(StdFlightLen_km,width=7,median,partial=T,na.rm=T),
AvgFlightDur_min_med_7day = rollapplyr(AvgFlightDur_min,width=7,median,partial=T,na.rm=T),
StdFlightDur_min_med_7day = rollapplyr(StdFlightDur_min,width=7,median,partial=T,na.rm=T),
Hometime_hrs_med_7day = rollapplyr(Hometime_hrs,width=7,median,partial=T,na.rm=T),
SigLocsVisited_med_7day = rollapplyr(SigLocsVisited,width=7,median,partial=T,na.rm=T),
SigLocEntropy_med_7day = rollapplyr(SigLocEntropy,width=7,median,partial=T,na.rm=T),
ProbPause_med_7day = rollapplyr(ProbPause,width=7,median,partial=T,na.rm=T),
CircdnRtn_med_7day = rollapplyr(CircdnRtn,width=7,median,partial=T,na.rm=T),
WkEndDayRtn_med_7day = rollapplyr(WkEndDayRtn,width=7,median,partial=T,na.rm=T),
RoG_km_med_7day = rollapplyr(RoG_km,width=7,median,partial=T,na.rm=T)) %>%
dplyr::select(person_id,contains("DistTravelled"),contains("MaxDiam"),
contains("MaxHomeDist"),contains("AvgFlightLen"),contains("StdFlightLen"),
contains("AvgFlightDur"),contains("StdFlightDur"),contains("Hometime"),
contains("SigLocsVisited"),contains("SigLocEntropy"),
contains("ProbPause"),contains("CircdnRtn"),
contains("WkEndDayRtn"),contains("RoG"),
local_date,days_from_start,utc_timecode)
##*************************##
####Export Resulting Data####
##*************************##
#export a unique csv file for each person.
for(person_id_cur in unique(as.character(metric_summary$person_id))){
person_sub_data <- metric_summary %>% filter(person_id == person_id_cur)
write.csv(person_sub_data,file=paste0(output,person_id_cur,"_gps_summaries.csv"))
}
#daily summary lags
write.csv(metric_summary_daily_lags,file=paste0(output,"GPS daily summaries with lags.csv"))
#weekly summary lags
write.csv(metric_summary_weekly_lags,file=paste0(output,"GPS weekly summaries with lags.csv"))
#Save results in RData file so that summary computation does not need to be repeated.
save(results,metric_summary,metric_summary_daily_lags,metric_summary_weekly_lags,
file=paste0(temp,"Example_GPS_summary.RData"))
library(tm)
#file path
train.space <- system.file("texts","20Newsgroups", "20news-bydate-train", "sci.space", package = 'tm')
train.auto <- system.file("texts","20Newsgroups", "20news-bydate-train", "rec.autos", package = 'tm')
test.space <- system.file("texts","20Newsgroups","20news-bydate-test","sci.space", package = 'tm')
test.auto <- system.file("texts","20Newsgroups","20news-bydate-test","rec.autos", package = 'tm')
#doc1 - sci.space, doc2 - rec.autos
Doc1.train.space <- DirSource(train.space)[1:200]
#file path
train.space <- system.file("texts","20Newsgroups", "20news-bydate-train", "sci.space", package = 'tm')
train.auto <- system.file("texts","20Newsgroups", "20news-bydate-train", "rec.autos", package = 'tm')
test.space <- system.file("texts","20Newsgroups","20news-bydate-test","sci.space", package = 'tm')
test.auto <- system.file("texts","20Newsgroups","20news-bydate-test","rec.autos", package = 'tm')
#doc1 - sci.space, doc2 - rec.autos
Doc1.train.space <- DirSource(train.space)[1:200]
Doc2.test.space <- DirSource(test.space)[1:200]
Doc2.train.auto <- DirSource(train.auto)[1:200]
Doc1.test.auto <- DirSource(test.auto)[1:200]
#corpus
Doc <- c(Doc1.train.space, Doc2.test.space, Doc2.train.auto, Doc1.test.auto)
corpus.doc <- Corpus(URISource(Doc), readerControl = list(reader=readPlain))
doc
Doc
corpus.doc
#remove white space
corpus.doc.temp <- tm_map(corpus.doc, stripWhitespace)
#Convert   to   Lowercase
corpus.doc.temp <- tm_map(corpus.doc.temp, tolower)
#Remove   Punctuation
corpus.doc.temp <- tm_map(corpus.doc.temp, removePunctuation)
#remove numbers
corpus.doc.temp <- tm_map(corpus.doc.temp, removeNumbers)
dtm <- as.matrix(DocumentTermMatrix(corpus.doc.temp, control = list(wordLengths=c(1,Inf), bounds=list(global=c(1, Inf)))))
freq <- colSums(dtm)
dtm
#Convert   to   Lowercase
corpus.doc.temp <- tm_map(corpus.doc.temp, getTransformations(tolower))
#remove white space
corpus.doc.temp <- tm_map(corpus.doc, stripWhitespace)
#Convert   to   Lowercase
corpus.doc.temp <- tm_map(corpus.doc.temp, content_transformer(tolower))
#Remove   Punctuation
corpus.doc.temp <- tm_map(corpus.doc.temp, removePunctuation)
#remove numbers
corpus.doc.temp <- tm_map(corpus.doc.temp, removeNumbers)
dtm <- as.matrix(DocumentTermMatrix(corpus.doc.temp, control = list(wordLengths=c(1,Inf), bounds=list(global=c(1, Inf)))))
freq <- colSums(dtm)
dtm
freq
#corpus
Doc <- c(Doc1.train.space, Doc2.test.space, Doc2.train.auto, Doc1.test.auto)
corpus.doc <- Corpus(URISource(Doc), readerControl = list(reader=readPlain))
#remove white space
corpus.doc.temp <- tm_map(corpus.doc, stripWhitespace)
#Convert   to   Lowercase
corpus.doc.temp <- tm_map(corpus.doc.temp, content_transformer(tolower))
#Remove   Punctuation
corpus.doc.temp <- tm_map(corpus.doc.temp, removePunctuation)
#remove numbers
corpus.doc.temp <- tm_map(corpus.doc.temp, removeNumbers)
dtm <- as.matrix(DocumentTermMatrix(corpus.doc.temp, control = list(wordLengths=c(1,Inf), bounds=list(global=c(1, Inf)))))
freq <- colSums(dtm)
ord <- order(freq, decreasing = TRUE)
frequency <- freq[ord]
Freq.Percent <- frequency/sum(frequency)*100
#Zipf variables
TotalNum_Words <- dim(dtm)[2]
dim(dtm)
rank <- 1:TotalNum_Words
x <- log(rank)
y <- log(frequency)
rank
x
y
data <- data.frame(X=x,Y=y)
plot(data, col = "red")
data
#Linear model
model <- lm(Y ~ X, data)
model$coefficients
s <- round(model$coefficients[2],4)
predictedY <- predict(model,data)
lines(data$X, predictedY, col = "blue", pch =4)
Title <- paste0("Zipf's law (s=",s,") for Newsgroups")
title(Title)
Zipf_plot(dtm, main = Title, type = "p", col = "red")
yy <- cumsum(Freq.Percent)
xx<- rank
data1 <- data.frame(X=xx,Y=yy)
plot(data1, col = "red", xlab = "Rank of unique words", ylab = "cumulative Percent %")
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted")
Title <- "Percent distribution of words in text"
title(Title)
#percentage of single words
ix <- which(frequency<2)
Single.Words <- sum(frequency[ix]/TotalNum_Words*100)
Title1 <- "The 100 most frequent words comprise 50% of entire text!"
Title2 <- "Half (50%) of any text is the 100 same (most Frequent) words!"
plot(data1[1:100,], col ="red", xlab = "Rank", ylab = "Cumulative percent%")
Title <- paste(Title1,Title2,sep = "\n")
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted")
title(main = Title, sub ="The 100 most freq words")
freq
ord
frequency
ord
frequency
frequency[1:20]
#Find their frequency
top20Frequency <- frequency[1:20]
top20Frequency
Freq.Percent <- top20Frequency/sum(frequency)*100
Freq.Percent
top20FreqPercent <- top20Frequency/sum(frequency)*100
top20FreqPercent
#4.Create a linear model from the   data and find what is Zipf’s  exponent for this dataset.
#Familiarize yourself with the R fitting function “  lm   ()” from “stats” package   .
#Use   lm   (Y ~ X , data   )
#Take X to be the natural logarithm   of the   Frequency
#Take Y to be the   natural logarithm   of the   Rank (the order from most to list popular words)
totalWords <- dim(dtm)[2]
rank <- 1:totalWords
x <- log(rank)
y <- log(frequency) #all frequency in decresing order
data <- data.frame(X=x,Y=y)
#Linear model
model <- lm(Y ~ X, data)
#Linear model
linearModel <- lm(Y ~ X, data)
linearModel
s <- round(linearModel$coefficients[2],4)
predictedY <- predict(linearModel,data)
lines(data$X, predictedY, col = "blue", pch =4)
Title <- paste0("Zipf's law (s=",s,") for Newsgroups")
title(Title)
round(linearModel$coefficients[2],4)
x_value <- linearModel$coefficients[2]
predicted_y_value <- predict(linearModel,data)
x_value
predicted_y_value
cozf=coef(lm(Y~X))
cozf=coef(lm(Y ~ X, data))
cozf
zipf.f = function(x) exp(cozf[[1]] + cozf[2]*log(x))
zipf.f
exp(cozf[[1]] + cozf[2]*log(x))
data <- data.frame(X=x,Y=y)
plot(data, col="red")
#Linear model
linearModel <- lm(Y ~ X, data)
linearModel
s <- round(linearModel$coefficients[2],4)
predictedY <- predict(linearModel,data)
lines(data$X, predictedY, col = "blue", pch =4)
Title <- paste0("Zipf's law (s=",s,") for Newsgroups")
title(Title)
Zipf_plot(dtm, main = Title, type = "p", col = "red")
s
#percentage of single words
ix <- which(frequency<2)
Single.Words <- sum(frequency[ix]/totalWords*100)
plot(Single.Words)
yy <- cumsum(Freq.Percent)
xx<- rank
data1 <- data.frame(X=xx,Y=yy)
plot(data1, col = "red", xlab = "Rank of unique words", ylab = "cumulative Percent %")
frequency <- freq[orderedIndex] #all frequencies in order
Freq.Percent <- frequency/sum(frequency)*100
#20 most frequent words
freq <- colSums(dtm)
orderedIndex <- order(freq, decreasing = TRUE)
frequency <- freq[orderedIndex] #all frequencies in order
Freq.Percent <- frequency/sum(frequency)*100
top20Frequency <- frequency[1:20]
#Find their frequency
top20Frequency
yy <- cumsum(Freq.Percent)
xx<- rank
data1 <- data.frame(X=xx,Y=yy)
plot(data1, col = "red", xlab = "Rank of unique words", ylab = "cumulative Percent %")
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted")
Title <- "Percent distribution of words in text"
title(Title)
#percentage of single words
ix <- which(frequency<2)
Single.Words <- sum(frequency[ix]/totalWords*100)
#percentage of single words
ix <- which(frequency<2)
Single.Words <- sum(frequency[ix]/totalWords*100)
Single.Words
Title1 <- "The 100 most frequent words comprise 50% of entire text!"
Title2 <- "Half (50%) of any text is the 100 same (most Frequent) words!"
Title <- paste(Title1,Title2,sep = "\n")
plot(data1[1:100,], col ="red", xlab = "Rank", ylab = "Cumulative percent%")
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted")
title(main = Title, sub ="The 100 most freq words")
source <- url("www.wordcount.org")
source <- url(www.wordcount.org)
source <- url('www.wordcount.org')
source <- readtext('www.wordcount.org')
source <- readtext(www.wordcount.org)
source <- readtext("www.wordcount.org")
conn <- file(url(www.wordcount.org ), "r")
conn <- file(url("www.wordcount.org"), "r")
conn <- file(url('www.wordcount.org'), "r")
conn <- file('www.wordcount.org', "r")
conn <- file("www.wordcount.org", "r")
url2fetch <- 'www.wordcount.org'
url_content <- getURL(url2fetch)
file(url2fetch, "r")
library(tm)
a  <-Corpus(DirSource("www.wordcount.org"), readerControl = list(language="lat"))
my_data <- read.delim("www.wordcount.org")
head(my_data)
my_data <- read.delim("www.wordcount.org")
frequency
which(frequency == 862)
which(frequency = 862)
frequency(862)
which(frequency = "military")
freq[862]
frequency[862]
dtm
dim(dtm)
rank
predictedY[862]
X[862]
data$X[862]
data[862]
data$Y[862]
which(predictedY == 6.759255)
which(predictedY == 3.465736)
which(predictedY == 'profit')
which(predictedY == "profit")
which(predictedY = "profit")
predictedY
predictedY$names
predictedY$values
predictedY.values
names(predictedY)
which(names(predictedY) == "military")
rank[469]
predictedY[469]
predict(linearModel,4.145256)
x = c(469)
predict(linearModel,data.frame(X = x))
predictedY
linearModel
predictedY[469]
rank[862]
dtm[862]
11.426 - (1.184*862)
11.426 - (1.184*469)
names(frequency) == "military"
which(names(frequency) == "military")
freq[469]
freq[862]
which(names(freq) == "military")
which(rank == 862)
x
log(rank)
ranks <- log(rank)
ranks[862]
ranks[469]
11.426 - (1.184*6.759255)
11.426 - (1.184*6.150603)
ranks[878]
names(freq) = "military"
which(names(freq) == "military")
which(names(frequency) == "military")
#20 most frequent words
freq <- colSums(dtm)
which(names(freq) == "military")
frequency[469]
freq[10427]
rank[469]
#military
#57
x <- log(862)
data <- data.frame(X=x)
predictedY <- predict(linearModel,data)
predictedY
which(predictedY == 3.424805)
which(names(frequency) == "military")
#[1] 469
frequency[469]
#military
#57
x <- log(862)
data <- data.frame(X=x)
predicted_Y <- predict(linearModel,data)
predicted_Y
predictedY <- predict(linearModel,data)
lines(data$X, predictedY, col = "blue", pch =4)
which(names(predictedY) == "military")
predictedY
dtm <- as.matrix(DocumentTermMatrix(corpus.doc.temp,
control = list(wordLengths=c(1,Inf),
bounds=list(global=c(1, Inf)))))
#20 most frequent words
freq <- colSums(dtm)
orderedIndex <- order(freq, decreasing = TRUE)
frequency <- freq[orderedIndex] #all frequencies in order
Freq.Percent <- frequency/sum(frequency)*100
top20Frequency <- frequency[1:20]
#Find their frequency
top20Frequency
top20FreqPercent <- top20Frequency/sum(frequency)*100
top20FreqPercent
#4.Create a linear model from the   data and find what is Zipf’s  exponent for this dataset.
#Familiarize yourself with the R fitting function “  lm   ()” from “stats” package   .
#Use   lm   (Y ~ X , data   )
#Take X to be the natural logarithm   of the   Frequency
#Take Y to be the   natural logarithm   of the   Rank (the order from most to list popular words)
totalWords <- dim(dtm)[2]
rank <- 1:totalWords
x <- log(rank)
y <- log(frequency) #all frequency in decresing order
data <- data.frame(X=x,Y=y)
plot(data, col="red")
#Linear model
linearModel <- lm(Y ~ X, data)
linearModel
s <- round(linearModel$coefficients[2],4)
predictedY <- predict(linearModel,data)
lines(data$X, predictedY, col = "blue", pch =4)
Title <- paste0("Zipf's law (s=",s,") for Newsgroups")
title(Title)
Zipf_plot(dtm, main = Title, type = "p", col = "red")
yy <- cumsum(Freq.Percent)
xx<- rank
data1 <- data.frame(X=xx,Y=yy)
plot(data1, col = "red", xlab = "Rank of unique words", ylab = "cumulative Percent %")
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted")
Title <- "Percent distribution of words in text"
title(Title)
#percentage of single words
ix <- which(frequency<2)
Single.Words <- sum(frequency[ix]/totalWords*100)
Single.Words
Title1 <- "The 100 most frequent words comprise 50% of entire text!"
Title2 <- "Half (50%) of any text is the 100 same (most Frequent) words!"
Title <- paste(Title1,Title2,sep = "\n")
plot(data1[1:100,], col ="red", xlab = "Rank", ylab = "Cumulative percent%")
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted")
title(main = Title, sub ="The 100 most freq words")
which(names(frequency) == "military")
#[1] 469
frequency[469]
#military
#57
x <- log(862)
#military
#57
x <- log(862)
data_x <- data.frame(X=x)
predicted_Y <- predict(linearModel,data_x)
predicted_Y
which(names(predictedY) == "military")
predictedY[469]
library(tm)
library(stats)
#file path
train.space <- system.file("texts","20Newsgroups", "20news-bydate-train", "sci.space", package = 'tm')
train.auto <- system.file("texts","20Newsgroups", "20news-bydate-train", "rec.autos", package = 'tm')
test.space <- system.file("texts","20Newsgroups","20news-bydate-test","sci.space", package = 'tm')
test.auto <- system.file("texts","20Newsgroups","20news-bydate-test","rec.autos", package = 'tm')
#doc1 - sci.space, doc2 - rec.autos
Doc1.train.space <- DirSource(train.space)[1:200]
Doc2.test.space <- DirSource(test.space)[1:200]
Doc2.train.auto <- DirSource(train.auto)[1:200]
Doc1.test.auto <- DirSource(test.auto)[1:200]
#corpus
Doc <- c(Doc1.train.space, Doc2.test.space, Doc2.train.auto, Doc1.test.auto)
corpus.doc <- Corpus(URISource(Doc), readerControl = list(reader=readPlain))
#Remove   Whitespace
corpus.doc.temp <- tm_map(corpus.doc, stripWhitespace)
#Convert   to   Lowercase
corpus.doc.temp <- tm_map(corpus.doc.temp, content_transformer(tolower))
#Remove   Punctuation
corpus.doc.temp <- tm_map(corpus.doc.temp, removePunctuation)
#Remove Numbers
corpus.doc.temp <- tm_map(corpus.doc.temp, removeNumbers)
dtm <- as.matrix(DocumentTermMatrix(corpus.doc.temp,
control = list(wordLengths=c(1,Inf),
bounds=list(global=c(1, Inf)))))
#20 most frequent words
freq <- colSums(dtm)
orderedIndex <- order(freq, decreasing = TRUE)
frequency <- freq[orderedIndex] #all frequencies in order
Freq.Percent <- frequency/sum(frequency)*100
top20Frequency <- frequency[1:20]
#Find their frequency
top20Frequency
top20FreqPercent <- top20Frequency/sum(frequency)*100
#4.Create a linear model from the   data and find what is Zipf’s  exponent for this dataset.
#Familiarize yourself with the R fitting function “  lm   ()” from “stats” package   .
#Use   lm   (Y ~ X , data   )
#Take X to be the natural logarithm   of the   Frequency
#Take Y to be the   natural logarithm   of the   Rank (the order from most to list popular words)
totalWords <- dim(dtm)[2]
top20FreqPercent
rank <- 1:totalWords
x <- log(rank)
y <- log(frequency) #all frequency in decresing order
data <- data.frame(X=x,Y=y)
plot(data, col="red")
#Linear model
linearModel <- lm(Y ~ X, data)
linearModel
s <- round(linearModel$coefficients[2],4)
predictedY <- predict(linearModel,data)
lines(data$X, predictedY, col = "blue", pch =4)
Title <- paste0("Zipf's law (s=",s,") for Newsgroups")
title(Title)
Zipf_plot(dtm, main = Title, type = "p", col = "red")
yy <- cumsum(Freq.Percent)
xx<- rank
data1 <- data.frame(X=xx,Y=yy)
plot(data1, col = "red", xlab = "Rank of unique words", ylab = "cumulative Percent %")
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted")
Title <- "Percent distribution of words in text"
title(Title)
#percentage of single words
ix <- which(frequency<2)
Single.Words <- sum(frequency[ix]/totalWords*100)
Single.Words
Title1 <- "The 100 most frequent words comprise 50% of entire text!"
Title2 <- "Half (50%) of any text is the 100 same (most Frequent) words!"
Title <- paste(Title1,Title2,sep = "\n")
plot(data1[1:100,], col ="red", xlab = "Rank", ylab = "Cumulative percent%")
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted")
title(main = Title, sub ="The 100 most freq words")
x <- log(862)
data_x <- data.frame(X=x)
predicted_Y <- predict(linearModel,data_x)
predicted_Y
which(names(predictedY) == "military")
#[1] 469
predictedY[469]
#military
#from the model the value is predicted to be 4.145256
#from the model the value is predicted to be 4.145256
#but with rank from word count its value is 3.424805
